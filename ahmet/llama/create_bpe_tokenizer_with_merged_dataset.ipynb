{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88ae870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentences = []\n",
    "\n",
    "with open(\"merged_pretrain_data_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        cleaned_line = line.strip()\n",
    "        list_of_sentences.append(cleaned_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48b134b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "llama32tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"], vocab_size=35000)\n",
    "\n",
    "llama32tokenizer.train_from_iterator(list_of_sentences, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b4e775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=llama32tokenizer,\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78fb993b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my_tokenizer/tokenizer_config.json',\n",
       " './my_tokenizer/special_tokens_map.json',\n",
       " './my_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.save_pretrained(\"./my_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13feb238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"my_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b495e291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35000"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e68f077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentences = []\n",
    "\n",
    "with open(\"merged_pretrain_data_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f872087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['muhteşem bir filmdi kesinlikle izlenmeye değer olarak görüyorum görüntü kalitesi ve kurgu çok iyiydi verilen puanlardan daha fazlasını hakettiğini düşünüyorum',\n",
       " 'İlk defa küfürsüz bir komedi filmi bence diğerleri kıskanıyorlar ben cem yılmazın filminde bile bu kadar eğlenmedim bu da aynen ıssız adam filmine benzeyecek giden herkes tavsiye ediyor önemli olan seyircinin beğenmnesi bence en önemli juri seyirci çıkan herkes filme 10 numara diyordu hele gürgen öz süperrrrrrrrrrrrrrr',\n",
       " 'müthiş',\n",
       " 'gerçekten çok daha iyisini bekliyodum evet erotik olması için çıplaklık olmasına gerek yok ama konu da beni vurmadı açıkçası yine de her Kubrick filmi gibi izlemeye değer çok şey bekleyip sadece beğendiğim bi film',\n",
       " 'unal15 arkadaşıma katılıyorum gereksiz bi film SAW serisi sevenleri kızdıracak bi film fragmanından berbat bi şey olduğu belli 0 1 10 bizim burda bir gün erkenden gösterime giriği için izleyipte puan veriyorum izlemediğim filme puan vermem adetim değildir',\n",
       " 'freestyle79 kardeşim film zaten sadist bir adamın insanlara hayatın değerini anlatma çabası üzerine kurulu Bu sadist adam onlara çeşitli tuzaklar kurup akıllarını kullanarak hayatta kalmalarını istiyor Bkz 1 film Kurtulan kız kendisine denileni yaptığı için kurtuldu Böyle bir film ve böyle bir sadist adam ortadayken insanlara kurduğu tuzakların sonucununun izleyiciye gösterilmemesi hem haksızlık olurdu hemde film türüne ve anlatmak istediğine hizmet ediyor olamazdı Kan ve vahşetin dozunda sorun var diyorsan 1 Zaten filmin adı testere 2 Yapımcılar da filmin yönetmeni de filmde kan ve vahşet yok demiyor aksine var diyor 3 diyelim ki sen bir manavsın ve çilek satıyorsun ama çilekleri sergilemeden müşteri istediği zaman gizli bir kutudan göstermeden paketleyip öyle veriyorsun Sen müşteri olsan senden çilek alırmısın İyi çalışmalar',\n",
       " 'beni fazlasıyla gerdi film beğendiğim filmlerden biri gerilim bilim kurgu psikoloji var işin içinde daha doğrusu filmde',\n",
       " 'izlemesenizde olur kesinlikle gidip bu filme para vermeyin ben yaptım bi hata sizde yapmayın',\n",
       " 'iyide olsa kötüde olsa rocky var sonuçta sahnede her zaman izlerim filmi izledim herkesede tavsiye ediyorum silvestır amcamda vücudunu iyi korumş ilerleyen yaşına rağmen',\n",
       " 'Serinin en kötü filmi ama serinin en kötü filmi bile izlerken insanı bu dünyadan alıp başka bir dünyaya götürüyor çok kaliteli bir film özellikle kale elden düşerken Gandalf ın süvarilerle orkların arasına daldığı sahne çok iyi 10 8 4']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "beab98af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3985,\n",
       " 5703,\n",
       " 2957,\n",
       " 14821,\n",
       " 2192,\n",
       " 18067,\n",
       " 4873,\n",
       " 4247,\n",
       " 4419,\n",
       " 4801,\n",
       " 11829,\n",
       " 4460,\n",
       " 9026,\n",
       " 2698,\n",
       " 23459,\n",
       " 2475,\n",
       " 6275,\n",
       " 4952]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.encode(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41920ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1742414"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4694750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "dicti = defaultdict(int)\n",
    "\n",
    "line_number = 0\n",
    "for line in lines:\n",
    "    tokens = fast_tokenizer.encode(line)\n",
    "    for token in tokens:\n",
    "        dicti[token] += 1\n",
    "    line_number += 1\n",
    "    print(f\"Processed line number: {line_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ec4c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort dicti by values in descending order\n",
    "dicti = dict(sorted(dicti.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2171d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"token_frequency.json\",\"w\") as f :\n",
    "    json.dump(dicti, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88cbbed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicti[1279]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids        token_num \n",
    "617-1094 = 478 \n",
    "1279-1751 =473 \n",
    "1800-1829 =30 \n",
    "1846-1892 =47 \n",
    "\n",
    "total = 1028 token\n",
    "2232 token by freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6070f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_ids= list(range(617,1095)) + list(range(1279,1752)) + list(range(1800,1830)) + list(range(1846,1893))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab6565cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"token_frequency.json\",\"w\") as f :\n",
    "    json.dump(dicti, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bd4b48f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[]\n",
    "\n",
    "k=0\n",
    "for i in reversed(list(dicti.keys())):\n",
    "    if (i not in delete_ids) and i>=24:\n",
    "        lst.append(i)\n",
    "        k+=1\n",
    "    if k==1204:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1d5769e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1204"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "658b264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat delete_ids and lst\n",
    "final_ids = delete_ids + lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7c15badc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2232"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b36da62",
   "metadata": {},
   "source": [
    "delete tokens from vocab and merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ae1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./my_tokenizer/tokenizer.json\",\"r\") as f:\n",
    "    tokenizer_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "682cef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = {}\n",
    "new_id = 0\n",
    "\n",
    "for token, old_id in tokenizer_json[\"model\"][\"vocab\"].items():\n",
    "    if old_id in final_ids:  # skip tokens we want to delete\n",
    "        continue\n",
    "    new_vocab[token] = new_id  # assign new consecutive ID\n",
    "    new_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3dc8e4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(new_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8dd44523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32313"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_json[\"model\"][\"merges\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d3e22c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_merges=[]\n",
    "\n",
    "for couple_list in tokenizer_json[\"model\"][\"merges\"]:\n",
    "    if couple_list[0] in new_vocab and couple_list[1] in new_vocab:\n",
    "        new_merges.append(couple_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3e59d830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32313"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6244b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json[\"model\"][\"vocab\"] =new_vocab\n",
    "tokenizer_json[\"model\"][\"merges\"] = new_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d3ef88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./my_tokenizer/tokenizer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tokenizer_json, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "412fe45f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Token `avsiye ` out of vocabulary at line 162081 column 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizerFast\n\u001b[0;32m----> 3\u001b[0m fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mPreTrainedTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_tokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2025\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2022\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2023\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2278\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2276\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2277\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2278\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2279\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2280\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2283\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:117\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[0;31mException\u001b[0m: Token `avsiye ` out of vocabulary at line 162081 column 1"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"my_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fbaea510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer_json[\"model\"][\"vocab\"].keys()).count(\"`avsiye `\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c91e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for listt in tokenizer_json[\"model\"][\"merges\"]:\n",
    "    if listt[0]==\"`avsiye\" or listt[1]==\" `\":\n",
    "        print(\"found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac9dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_tokenizer.push_to_hub(\"AhmetSemih/merged_dataset-32k-bpe-tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
