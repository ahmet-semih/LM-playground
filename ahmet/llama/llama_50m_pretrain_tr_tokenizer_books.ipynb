{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4d79831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # batchsize 64\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbb14242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ecdb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "tensors = load_file(\"all_books_token_ids.safetensors\")#ids from books dataset by turkish_tokenizer(github alibayram)\n",
    "token_ids = tensors['a']\n",
    "print(type(token_ids))  # should be <class 'torch.Tensor'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f40c617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd713898c91d4937aabfd3c540ab2d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "all_books_token_ids.safetensors:   0%|          | 0.00/745M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Download and cache the file from HF Hub\n",
    "filepath = hf_hub_download(\n",
    "    repo_id=\"AhmetSemih/tr_tokenizer_books_tokens\",\n",
    "    filename=\"all_books_token_ids.safetensors\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "# Load tensors\n",
    "tensors = load_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8f9bff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(token_ids))  # should be <class 'dict'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "837ffa37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2, 20938,     0,  ..., 20028, 31897,     3], dtype=torch.int16)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract list from tensor\n",
    "token_ids = tensors['a']\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "834d531a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "372679971"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cdbbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors import torch as sftorch\n",
    "\n",
    "model_path = hf_hub_download(repo_id=\"aliarda/llama-50M-randParams\", filename=\"llama-50M.safetensors\", local_dir=\"./models\")\n",
    "state_dict = sftorch.load_file(model_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90e1d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LlamaConfig():\n",
    "  def __init__(\n",
    "          self,\n",
    "          vocab_size: int = 32_768,\n",
    "          context_length: int = 512,\n",
    "          emb_dim: int = 256,\n",
    "          n_heads: int = 256,\n",
    "          n_layers: int = 20,\n",
    "          hidden_dim: int = 2048,\n",
    "          n_kv_groups: int = 64,\n",
    "          head_dim: int | None = None,\n",
    "          dtype: torch.dtype = torch.float32,\n",
    "          mlp_bias: bool = False,\n",
    "          rms_norm_eps: float = 1e-6,\n",
    "          bias: bool = False,\n",
    "          attention_bias: bool = False,\n",
    "        ):\n",
    "      self.vocab_size = vocab_size\n",
    "      self.max_position_embeddings = context_length\n",
    "      self.hidden_size = emb_dim\n",
    "      self.num_attention_heads = n_heads\n",
    "      self.num_hidden_layers = n_layers\n",
    "      self.num_key_value_heads = n_kv_groups\n",
    "      self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n",
    "      self.dtype = dtype\n",
    "      self.intermediate_size = hidden_dim\n",
    "      self.mlp_bias = mlp_bias\n",
    "      self.rms_norm_eps = rms_norm_eps\n",
    "      self.bias = bias\n",
    "      self.attention_bias = attention_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cde6fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "\n",
    "\n",
    "class LlamaRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        LlamaRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "def precompute_freqs_cis(dim:int, seq_len: int, theta: float=10000.0, device: torch.device = torch.device(\"cpu\")):\n",
    "  # Computing Theta value for each dim pair which is dim/2\n",
    "  freqs = 1.0 / (theta ** (torch.arange(0, dim, 2,device=device)[:(dim//2)].float()/dim))\n",
    "\n",
    "  # Computing range of positions(m) in the sequence\n",
    "  t = torch.arange(seq_len, dtype=torch.float32, device=device)\n",
    "\n",
    "  # freqs gives all the Theta value range for all the position of tokens in the sequence\n",
    "  freqs = torch.outer(t, freqs).to(device)\n",
    "\n",
    "  # This is the rotation matrix which needs to be converted to Polar form in order to perform rotation to the embedding\n",
    "  freqs_cis = torch.polar(torch.ones_like(freqs).to(device), freqs).to(device)\n",
    "  return freqs_cis\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis, x):\n",
    "  ndim = x.ndim\n",
    "  assert 0<=1<ndim\n",
    "  assert freqs_cis.shape == (x.shape[1],x.shape[-1]), \"the last two dimension of freqs_cis, x must match\"\n",
    "  shape = [d if i==1 or i==ndim-1 else 1 for i,d in enumerate(x.shape)]\n",
    "  return freqs_cis.view(*shape)\n",
    "\n",
    "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor, device: torch.device = torch.device(\"cpu\"))->Tuple[torch.Tensor, torch.Tensor]:\n",
    "  # Applying rotary positional encoding to both query and key embedding together\n",
    "  # First: The last dimension of xq and xk embedding needs to be reshaped to make it a pair. As rotation matrix is applied to each pair of dim.\n",
    "  # Next: convert both xq and xk to complex number as the rotation matrix is only applicable to complex number\n",
    "  xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2)).to(device)    #xq_:[bsz, seq_len, n_heads, head_dim/2]\n",
    "  xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2)).to(device)    #xk_:[bsz, seq_len, n_heads, head_dim/2]\n",
    "\n",
    "  # The rotation matrix(freqs_cis) dimensions across seq_len(dim=1) and head_dim(dim=3) should match with the embedding\n",
    "  # Also, the shape freqs_cis should be the same with xq and xk, hence change the shape of freqs_cis:[seq_len,head_dim] -> freqs_cis:[1,seq_len,1,head_dim]\n",
    "  freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "\n",
    "  #Finally, perform rotation operation by multiplying with freqs_cis.\n",
    "  #After the rotation is completed, convert both xq_out and xk_out back to real number and return\n",
    "  xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3).to(device) #xq_out:[bsz, seq_len, n_heads, head_dim]\n",
    "  xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3).to(device) #xk_out:[bsz, seq_len, n_heads, head_dim]\n",
    "  return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "def repeat_kv(x:torch.Tensor, n_rep: int)-> torch.Tensor:\n",
    "  bsz, seq_len, n_kv_heads, head_dim = x.shape\n",
    "  if n_rep == 1:\n",
    "    return x\n",
    "  return (\n",
    "      x[:,:,:,None,:]\n",
    "      .expand(bsz,seq_len,n_kv_heads,n_rep, head_dim)\n",
    "      .reshape(bsz,seq_len,n_kv_heads * n_rep, head_dim)\n",
    "  )\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n",
    "        self.act_fn = nn.SiLU() # nn.functional.silu ACT2FN[config.hidden_act]\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
    "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.k_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.o_proj = nn.Linear(\n",
    "            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        batch_size, seq_len, _ = hidden_states.shape\n",
    "        xq = self.q_proj(hidden_states)\n",
    "        xk = self.k_proj(hidden_states)\n",
    "        xv = self.v_proj(hidden_states)\n",
    "\n",
    "        xq = xq.view(batch_size, seq_len, self.num_attention_heads, self.head_dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)\n",
    "\n",
    "        # Compute rotation matrix and apply RoPE to queries and keys for for training.\n",
    "        freqs_cis = precompute_freqs_cis(dim=self.head_dim, seq_len=seq_len, device=hidden_states.device)\n",
    "\n",
    "        #xq[bsz,seq_len,n_heads, head_dim], xk[bsz,seq_len,n_heads, head_dim]\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis, device=hidden_states.device)\n",
    "\n",
    "        # Use repeat_kv function to make Keys,Values shape same as the queries shape\n",
    "        #keys[bsz,seq_len,n_heads,head_dim], #values[bsz,seq_len,n_heads,head_dim]\n",
    "        keys = repeat_kv(xk, self.num_key_value_groups) #keys[bsz,seq_len,n_heads,head_dim]\n",
    "        values = repeat_kv(xv, self.num_key_value_groups)\n",
    "\n",
    "        # To compute attention, we'll need to perform a transpose operation to reshape all queries, keys and values bring heads at dim 1 and seq at dim 2\n",
    "        xq = xq.transpose(1,2).contiguous()                  #xq[bsz,n_heads,seq_len,head_dim]\n",
    "        keys = keys.transpose(1,2).contiguous()              #keys[bsz,n_heads,seq_len,head_dim]\n",
    "        values = values.transpose(1,2).contiguous()          #values[bsz,n_heads,seq_len,head_dim]\n",
    "\n",
    "        # Using Scaled Dot Product Attention to compute attention score and attention output\n",
    "        attn_out = F.scaled_dot_product_attention(\n",
    "            xq, keys, values,\n",
    "            attn_mask=None,\n",
    "            is_causal=True\n",
    "        ) #attn_out[bsz, n_heads, seq_len, head_dim]\n",
    "\n",
    "        # Merge heads back\n",
    "        output = attn_out.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "\n",
    "        # shape: output [bsz,seq_len,dim]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = LlamaAttention(config=config, layer_idx=layer_idx)\n",
    "\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        hidden_states = self.self_attn(hidden_states)\n",
    "        hidden_states = hidden_states + residual\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = hidden_states + residual\n",
    "        return hidden_states\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig, embedding: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        # self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, dtype=config.dtype)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        # self.rotary_emb = LlamaRotaryEmbedding(config=config)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        hidden_states = self.embed_tokens(input_ids)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig, embedding: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config, embedding)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=config.bias)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        hidden_states = self.model(input_ids)\n",
    "        return self.lm_head(hidden_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5a17750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32768, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-19): 20 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=False)\n",
       "          (v_proj): Linear(in_features=256, out_features=128, bias=False)\n",
       "          (o_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=256, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=256, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_config = LlamaConfig(\n",
    "    vocab_size=32768,\n",
    "    emb_dim=256,\n",
    "    context_length=256,\n",
    "    n_heads=128,\n",
    "    n_layers=20,\n",
    "    n_kv_groups=64,\n",
    "    hidden_dim=2048,\n",
    ")\n",
    "\n",
    "llama_model = LlamaForCausalLM(llama_config)\n",
    "llama_model = llama_model.to(device)\n",
    "llama_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bb61497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c176c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "pad_id = 5\n",
    "eos_id = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64a6d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, token_ids: list, context_length: int, stride: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        for i in range(0, len(token_ids) - context_length, stride):\n",
    "            input_chunk = token_ids[i:i + context_length]\n",
    "            target_chunk = token_ids[i + 1:i + context_length + 1]\n",
    "\n",
    "            # truncate if the chunk is longer than context_length\n",
    "            input_chunk = input_chunk[:context_length]\n",
    "            target_chunk = target_chunk[:context_length]\n",
    "\n",
    "            # pad the input and target chunks to context_length\n",
    "            input_chunk += [pad_id] * (context_length - len(input_chunk))\n",
    "            target_chunk += [pad_id] * (context_length - len(target_chunk))\n",
    "\n",
    "            # truncate if the chunk is longer than context_length\n",
    "            input_chunk = input_chunk[:context_length]\n",
    "            target_chunk = target_chunk[:context_length]\n",
    "\n",
    "            self.inputs.append(torch.tensor(input_chunk, dtype=torch.long))\n",
    "            self.targets.append(torch.tensor(target_chunk, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1afeb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(token_ids: list, context_len: int, stride: int, batch_size: int, shuffle: bool, device: str = \"cpu\"):\n",
    "    dataset = TextDataset(token_ids, context_len, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        generator=torch.Generator(device=device)\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4678d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(token_ids.tolist()[:5000], 256, 256, 64, False,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0c07e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from bitsandbytes) (1.14.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scipy->bitsandbytes) (2.0.2)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.42.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ac3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     18\u001b[0m checkpoint_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mllama_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:412\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_iterator\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_BaseDataLoaderIter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_SingleProcessDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:735\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, loader):\n\u001b[0;32m--> 735\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:668\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collate_fn \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mcollate_fn\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_sampler)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_seed \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    667\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 668\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    670\u001b[0m )\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mpersistent_workers\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from bitsandbytes.optim import AdamW8bit\n",
    "from huggingface_hub import upload_file\n",
    "from tqdm import tqdm\n",
    "from safetensors import torch as sftorch\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = AdamW8bit(llama_model.parameters(), lr=1e-3)\n",
    "\n",
    "save_interval = 4  # 2 hours in seconds\n",
    "last_save_time = time.time()\n",
    "\n",
    "num_epochs = 1\n",
    "checkpoint_num = 1\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    for X, Y in tqdm(train_dataloader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "        pred = llama_model(X)\n",
    "        loss = loss_fn(pred.flatten(0, 1), Y.flatten())\n",
    "        total_loss += loss.item()\n",
    "        last_loss = loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        del pred, loss, X, Y\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Push model to HF every save_interval seconds\n",
    "        if time.time() - last_save_time >= save_interval:\n",
    "            last_save_time = time.time()\n",
    "            # Save and upload using sftorch + HF\n",
    "            sftorch.save_file(llama_model.state_dict(), f\"llama_model_{epoch_idx}_{checkpoint_num}.safetensors\")\n",
    "            upload_file(\n",
    "                path_or_fileobj=f\"llama_model_{epoch_idx}_{checkpoint_num}.safetensors\",\n",
    "                repo_id=\"AhmetSemih/llama-50m-pretrained-books-tr_tokenizer\",\n",
    "                path_in_repo=\"llama-50m-pretrained-books-tr_tokenizer.safetensors\",\n",
    "                commit_message=f\"upload llama_model chunk: {checkpoint_num}, epoch: {epoch_idx}\"\n",
    "            )\n",
    "            print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Uploaded checkpoint {checkpoint_num}\")\n",
    "            checkpoint_num += 1\n",
    "\n",
    "    # Upload final model at the end of the epoch\n",
    "    sftorch.save_file(llama_model.state_dict(), f\"llama_model_{epoch_idx}_final.safetensors\")\n",
    "    upload_file(\n",
    "        path_or_fileobj=f\"llama_model_{epoch_idx}_final.safetensors\",\n",
    "        repo_id=\"AhmetSemih/llama-50m-pretrained-books-tr_tokenizer\",\n",
    "        path_in_repo=\"llama-50m-pretrained-books-tr_tokenizer.safetensors\",\n",
    "        commit_message=f\"Final upload epoch {epoch_idx}\"\n",
    "    )\n",
    "    print(f\"Epoch {epoch_idx} completed. Uploaded final checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, x: torch.Tensor, max_new_tokens: int): # top_k, top_p, temperature\n",
    "  tokens = x.detach().cpu().numpy().tolist()\n",
    "\n",
    "  for _ in range(max_new_tokens):\n",
    "    x = x.unsqueeze(0).to(device)\n",
    "    out = model.forward(x)\n",
    "    out = out.squeeze(0)\n",
    "    probs = torch.softmax(out[-1], dim=-1)\n",
    "    _, max_index = torch.max(probs, dim=-1)\n",
    "    tokens.append(max_index.item())\n",
    "    if max_index == eos_id or len(tokens) > context_len: # <eos> and max context length\n",
    "      break\n",
    "\n",
    "    x = torch.tensor(tokens)\n",
    "\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92652237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04db07ea41f04177880e202b2fd3225b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e14320",
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23fb8549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a0b3e5121e4df3866335678ff60450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/745M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/AhmetSemih/tr_tokenizer_books_tokens/commit/0f9160a41054713236b23cdd35da829e3707b090', commit_message='Upload all_books_token_ids.safetensors with huggingface_hub', commit_description='', oid='0f9160a41054713236b23cdd35da829e3707b090', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/AhmetSemih/tr_tokenizer_books_tokens', endpoint='https://huggingface.co', repo_type='dataset', repo_id='AhmetSemih/tr_tokenizer_books_tokens'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_file(\n",
    "    path_or_fileobj=\"all_books_token_ids.safetensors\",\n",
    "    repo_id=\"AhmetSemih/tr_tokenizer_books_tokens\",\n",
    "    path_in_repo=\"all_books_token_ids.safetensors\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
